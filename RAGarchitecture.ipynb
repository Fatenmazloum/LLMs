{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d43222",
   "metadata": {},
   "source": [
    "RAG architecture  retrieves facts from an external knowledge(website,dataset..) and helps pretrained models generate more accurate and reduce hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2d0db",
   "metadata": {},
   "source": [
    "RAG (Retrieval-Augmented Generation) is a method that enhance chatbots by minimizing hallucinations, especially when dealing with new products or specific data that  AI model like ChatGPT might not know about. Using langchain framework and RAG method enhance chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe638fe",
   "metadata": {},
   "source": [
    "Vector database(handles large data as nmerical values not text) it helps machine learning  model identify similar objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19447e2a",
   "metadata": {},
   "source": [
    "1. Store Documents:\n",
    "\n",
    "You convert your documents (manuals, FAQs, etc.) into embeddings\n",
    "\n",
    "Save them in a vector database (like FAISS, Pinecone, Weaviate)\n",
    "\n",
    "2. User Asks a Question:\n",
    "\n",
    "The question is also converted into an embedding\n",
    "\n",
    "3. Retrieve:\n",
    "\n",
    "The system searches the vector database for documents that are most similar to the customerâ€™s question \n",
    "\n",
    "4. Generate:\n",
    "\n",
    "The LLM uses that retrieved info to generate a natural answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9faa36e",
   "metadata": {},
   "source": [
    "Ollama command-line tool helps you run chatbot locally (on your machine) and run LLM models(llama), meaning no cloud-based dependency(chatgpt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6b2c9",
   "metadata": {},
   "source": [
    "The API helps send the customerâ€™s question (converted into embeddings) to the vector database, and then it retrieves the most relevant information based on similarit. The API actually manages the communication between the frontend (chatbot interface) and the backend (which includes the vector database and language model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8240abc",
   "metadata": {},
   "source": [
    "Frontend: The chatbot interface where users interact, ask questions, and receive answers. This is visible to the user.\n",
    "\n",
    "Backend: The vector database where information is stored and retrieved and LLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584f788",
   "metadata": {},
   "source": [
    "The LLM  is the brain of your chatbot â€” The LLM in this process is not responsible for understanding the question in the way a human would. Instead, its role is to generate a human-like answer based on the text retrieved by vector database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f58cca",
   "metadata": {},
   "source": [
    "ðŸŸ¢ BERT is used when you need to understand language (like analyzing sentiment).\n",
    "\n",
    "ðŸŸ¢ LLaMA is used when you need to generate language (like creating responses)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
